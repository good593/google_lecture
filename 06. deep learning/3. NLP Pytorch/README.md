# [파이토치 한국어 튜토리얼](https://tutorials.pytorch.kr/)
- torchtext로 텍스트 분류하기
- 기초부터 시작하는 NLP: 시퀀스-투-시퀀스 네트워크와 어텐션을 이용한 번역
- 기초부터 시작하는 NLP: 문자-단위 RNN으로 이름 생성하기
- 기초부터 시작하는 NLP: 문자-단위 RNN으로 이름 분류하기
- nn.Transformer와 TorchText로 시퀀스-투-시퀀스 모델링하기
- Fast Transformer Inference with Better Transformer

# [Microsoft](https://learn.microsoft.com/en-us/training/modules/intro-natural-language-processing-pytorch/)
- Introduction
- Representing text as Tensors
- Bag of Words and TF-IDF
- Trepresent words with embeddings
- Capture patterns with recurrent neural networks
- Generate text with recurrent networks
- Summary

# [Githup 예제](https://github.com/graykode/nlp-tutorial)
- Basic Embedding Model
    - NNLM(Neural Network Language Model)
    - Word2Vec(Skip-gram)
    - FastText(Application Level)
- CNN(Convolutional Neural Network)
    - TextCNN
- RNN(Recurrent Neural Network)
    - TextRNN
    - TextLSTM
    - Bi-LSTM
- Attention Mechanism
    - Seq2Seq
    - Seq2Seq with Attention
    - Bi-LSTM with Attention
- Model based on Transformer
    - The Transformer
    - BERT

